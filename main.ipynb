{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Concatenate, BatchNormalization, Input, Activation, Dropout # type: ignore\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import read_data\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(filters, strides):\n",
    "    return Conv2D(filters=filters, kernel_size=3, strides=strides, activation='relu', padding='valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eye_conv_blocks(inputs) :\n",
    "    conv1 = conv_block(filters=64, strides=1)(inputs)\n",
    "    conv2 = conv_block(filters=64, strides=2)(conv1)\n",
    "    conv3 = conv_block(filters=128, strides=1)(conv2)\n",
    "    conv4 = conv_block(filters=128, strides=2)(conv3)\n",
    "    conv5 = conv_block(filters=256, strides=1)(conv4)\n",
    "    conv6 = conv_block(filters=256, strides=2)(conv5)\n",
    "    return conv6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def far_net() :\n",
    "    # Aliran Pertama - Mata Kiri\n",
    "    left_eye_input = Input(shape=(36,60,1))\n",
    "    first_stream = eye_conv_blocks(left_eye_input)\n",
    "    left_eye_features = Flatten()(first_stream)\n",
    "    left_eye_features = Dense(500, activation='relu')(left_eye_features)\n",
    "    \n",
    "    # Aliran Kedua - Mata Kanan\n",
    "    right_eye_input = Input(shape=(36,60,1))\n",
    "    second_stream = eye_conv_blocks(right_eye_input)\n",
    "    right_eye_features = Flatten()(second_stream)\n",
    "    right_eye_features = Dense(500, activation='relu')(right_eye_features)\n",
    "\n",
    "    # Aliran Ketiga - Wajah\n",
    "    face_input = Input(shape=(224,224,3))\n",
    "    face_features = Conv2D(96, kernel_size=(11,11), strides=(4,4))(face_input)\n",
    "    face_features = BatchNormalization()(face_features)\n",
    "    face_features = Activation(\"relu\")(face_features)\n",
    "    face_features = MaxPooling2D(pool_size=(3,3), strides=(2,2))(face_features)\n",
    "    \n",
    "    face_features = Conv2D(256, kernel_size=(5,5), padding='same')(face_features)\n",
    "    face_features = BatchNormalization()(face_features)\n",
    "    face_features = Activation(\"relu\")(face_features)\n",
    "    face_features = MaxPooling2D(pool_size=(3,3), strides=(2,2))(face_features)\n",
    "    \n",
    "    face_features = Conv2D(384, kernel_size=(3,3), padding='same')(face_features)\n",
    "    face_features = BatchNormalization()(face_features)\n",
    "    face_features = Activation(\"relu\")(face_features)\n",
    "    face_features = Conv2D(384, kernel_size=(3,3), padding='same')(face_features)\n",
    "    face_features = BatchNormalization()(face_features)\n",
    "    face_features = Activation(\"relu\")(face_features)\n",
    "    face_features = Conv2D(256, kernel_size=(3,3), padding='same')(face_features)\n",
    "    face_features = BatchNormalization()(face_features)\n",
    "    face_features = Activation(\"relu\")(face_features)\n",
    "    face_features = MaxPooling2D(pool_size=(3,3), strides=(2,2))(face_features)\n",
    "    \n",
    "    face_features = Flatten()(face_features)\n",
    "    face_features = Dense(4096, activation='relu')(face_features)\n",
    "    face_features = Dropout(0.5)(face_features)\n",
    "    face_features = Dense(4096, activation='relu')(face_features)\n",
    "    face_features = Dropout(0.5)(face_features)\n",
    "    face_features = Dense(500, activation='relu')(face_features)\n",
    "    \n",
    "    #Penggabungan Fitur\n",
    "    concatenated_features = Concatenate()([left_eye_features,right_eye_features, face_features])\n",
    "    outputs = Dense(6)(concatenated_features)\n",
    "    \n",
    "    model = keras.Model(inputs=[left_eye_input,right_eye_input,face_input], outputs=outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_net():\n",
    "    \n",
    "    # Aliran Pertama - Mata Kiri\n",
    "    left_eye_input = Input(shape=(36,60,1))\n",
    "    first_stream = eye_conv_blocks(left_eye_input)\n",
    "    left_eye_features = Flatten()(first_stream)\n",
    "    left_eye_features = Dense(1000, activation='relu')(left_eye_features)\n",
    "    left_eye_features = Dense(500, activation='relu')(left_eye_features)\n",
    "    \n",
    "    # Aliran Kedua - Mata Kanan\n",
    "    right_eye_input = Input(shape=(36,60,1))\n",
    "    second_stream = eye_conv_blocks(right_eye_input)\n",
    "    right_eye_features = Flatten()(second_stream)\n",
    "    right_eye_features = Dense(1000, activation='relu')(right_eye_features)\n",
    "    right_eye_features = Dense(500, activation='relu')(right_eye_features)\n",
    "    \n",
    "    concatenated_features = Concatenate()([left_eye_features,right_eye_features])\n",
    "    output = Dense(2, activation='softmax')(concatenated_features)\n",
    "    \n",
    "    model = keras.Model(inputs=[left_eye_input, right_eye_input], outputs=output)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_batches(data, batch_size):\n",
    "    batches = []\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch = data[i:i+batch_size]\n",
    "        batches.append(zip(*batch))\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def angular_err(v1, v2):\n",
    "\tv1xv2 = tf.reduce_sum(v1*v2,1)\n",
    "\tv1_len = tf.cast(tf.sqrt(tf.reduce_sum(tf.square(v1), 1)),dtype=tf.float32)\n",
    "\tv2_len = tf.cast(tf.sqrt(tf.reduce_sum(tf.square(v2), 1)),dtype=tf.float32)\n",
    "\t\n",
    "\tval = v1xv2/((v1_len* v2_len ) + 1e-10)\n",
    " \n",
    "\tdegree = tf.acos(val)\n",
    "\treturn degree * 180/ math.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fare_loss(left_err, right_err, probs):\n",
    "    \n",
    "            \n",
    "    far_err = ((2 * left_err * right_err ) + 1e-10)  / ((left_err + right_err) + 1e-10)\n",
    "    # print(f\"FAR-err -> {far_err}\")\n",
    "    \n",
    "    avg_err = (left_err + right_err) /2\n",
    "    \n",
    "    n = tf.cast(tf.less_equal(left_err, right_err), tf.float32)\n",
    "    \n",
    "    # print(f\"N -> {n}\")\n",
    "    \n",
    "    squared_distance = tf.reduce_sum(tf.square(left_err - right_err), axis=-1)\n",
    "    \n",
    "    e_loss = - (n * squared_distance * tf.math.log(probs[:,0]) + (1 - n) * squared_distance * tf.math.log(probs[:,1]))\n",
    "    \n",
    "    weight = (1 + (2 * n - 1) * probs[:,0] + (1 - 2 * n) * probs[:,1]) / 2\n",
    "    \n",
    "    # print(f\"W --> {weight}\")\n",
    "    \n",
    "    far_loss = weight * far_err + (1 - weight) * 0.1 * avg_err\n",
    "    \n",
    "    return far_loss, e_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(ds_train, ds_test, batch_size=100, epochs=1, dataset_type='original') :\n",
    "    \n",
    "    ds_train = list(zip(ds_train[0],ds_train[1],ds_train[2],ds_train[3]))\n",
    "    ds_test = list(zip(ds_test[0],ds_test[1],ds_test[2],ds_test[3]))\n",
    "    \n",
    "    far_net_model = far_net()\n",
    "    e_net_model = e_net()\n",
    "    \n",
    "    far_optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "    e_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
    "    \n",
    "    train_far_loss_metric = tf.keras.metrics.Mean(name='train_far_loss')\n",
    "    train_e_loss_metric = tf.keras.metrics.Mean(name='train_e_loss')\n",
    "    train_angular_error_metric = tf.keras.metrics.Mean(name='train_angular_error')\n",
    "    train_probability_accuracy = tf.keras.metrics.BinaryAccuracy(name='train_probability_accuracy')\n",
    "    \n",
    "    test_far_loss_metric = tf.keras.metrics.Mean(name='test_far_loss')\n",
    "    test_e_loss_metric = tf.keras.metrics.Mean(name='test_e_loss')\n",
    "    test_angular_error_metric = tf.keras.metrics.Mean(name='test_angular_error')\n",
    "    test_probability_accuracy = tf.keras.metrics.BinaryAccuracy(name='test_probability_accuracy')\n",
    "    \n",
    "    train_far_losses = []\n",
    "    train_e_losses = []\n",
    "    train_choose_accs = []\n",
    "    train_angular_errs = []\n",
    "    \n",
    "    test_far_losses = []\n",
    "    test_e_losses = []\n",
    "    test_choose_accs = []\n",
    "    test_angular_errs = []\n",
    "    \n",
    "    \n",
    "    def train_step(l_eyes, r_eyes, faces, labels):\n",
    "        with tf.GradientTape() as far_tape, tf.GradientTape() as e_tape :\n",
    "                \n",
    "            gaze_preds = far_net_model([l_eyes, r_eyes, faces], training=True)\n",
    "            probs = e_net_model([l_eyes, r_eyes], training=True)\n",
    "                        \n",
    "            # print(f\"Gaze  --> {gaze_preds[:,:3]}\")\n",
    "            # print(f\"Label --> {labels[:,:3]}\")\n",
    "            # print(f\"Gaze  --> {gaze_preds[:,3:]}\")\n",
    "            # print(f\"Label --> {labels[:,3:]}\")\n",
    "            \n",
    "            left_err = angular_err(gaze_preds[:,:3],labels[:,:3])\n",
    "            right_err = angular_err(gaze_preds[:,3:],labels[:,3:])\n",
    "            \n",
    "            # print(f\"Left Err --> {left_err}\")\n",
    "            # print(f\"Right Err --> {right_err}\")\n",
    "            \n",
    "            far_loss, e_loss = fare_loss(left_err, right_err, probs)\n",
    "            print(f\"FAR loss --> {tf.reduce_mean(far_loss)}\")\n",
    "            print(f\"E loss --> {tf.reduce_mean(e_loss)}\")\n",
    "            \n",
    "        gradients1 = far_tape.gradient(far_loss, far_net_model.trainable_variables)\n",
    "        gradients2 = e_tape.gradient(e_loss, e_net_model.trainable_variables)\n",
    "            \n",
    "        # print(f\"GRAD --> {gradients1}\")\n",
    "        # print(f\"GRAD --> {gradients2}\")\n",
    "        \n",
    "        far_optimizer.apply_gradients(zip(gradients1, far_net_model.trainable_variables))\n",
    "        e_optimizer.apply_gradients(zip(gradients2, e_net_model.trainable_variables))\n",
    "        \n",
    "        choose_preds = tf.cast(tf.less_equal(probs[:,0], probs[:,1]), tf.int16) \n",
    "        choose_labels = tf.cast(tf.greater_equal(left_err, right_err), tf.int16)\n",
    "        \n",
    "        angular_errors = [ left_err[i] if i == 0 else right_err[i] for i in choose_preds ]\n",
    "        \n",
    "        train_far_loss_metric(far_loss)\n",
    "        train_e_loss_metric(e_loss)\n",
    "        train_angular_error_metric(angular_errors)\n",
    "        train_probability_accuracy(choose_labels, choose_preds)\n",
    "    \n",
    "    def test_step(l_eyes, r_eyes, faces, labels):\n",
    "        gaze_preds = far_net_model([l_eyes, r_eyes, faces], training=False)\n",
    "        probs = e_net_model([l_eyes, r_eyes], training=False)\n",
    "        \n",
    "        # print(f\"Gaze  --> {gaze_preds[:,:3]}\")\n",
    "        # print(f\"Label --> {labels[:,:3]}\")\n",
    "        # print(f\"Gaze  --> {gaze_preds[:,3:]}\")\n",
    "        # print(f\"Label --> {labels[:,3:]}\")\n",
    "        \n",
    "        left_err = angular_err(gaze_preds[:,:3],labels[:,:3])\n",
    "        right_err = angular_err(gaze_preds[:,3:],labels[:,3:])\n",
    "        \n",
    "        # print(f\"Left Err --> {left_err}\")\n",
    "        # print(f\"Right Err --> {right_err}\")\n",
    "        \n",
    "        far_loss, e_loss = fare_loss(left_err, right_err, probs)\n",
    "        \n",
    "        choose_preds = tf.cast(tf.less_equal(probs[:,0], probs[:,1]), tf.int16) \n",
    "        choose_labels = tf.cast(tf.greater_equal(left_err, right_err), tf.int16)\n",
    "        \n",
    "        angular_errors = [ left_err[i] if i == 0 else right_err[i] for i in choose_preds ]\n",
    "        \n",
    "        test_far_loss_metric(far_loss)\n",
    "        test_e_loss_metric(e_loss)\n",
    "        test_angular_error_metric(angular_errors)\n",
    "        test_probability_accuracy(choose_labels, choose_preds)\n",
    "    \n",
    "    for epoch in range(1,epochs + 1) :\n",
    "        print(f\"Epoch {epoch} -- START\")\n",
    "        \n",
    "        train_far_loss_metric.reset_state()\n",
    "        train_e_loss_metric.reset_state()\n",
    "        train_angular_error_metric.reset_state()\n",
    "        train_probability_accuracy.reset_state()\n",
    "        \n",
    "        test_far_loss_metric.reset_state()\n",
    "        test_e_loss_metric.reset_state()\n",
    "        test_angular_error_metric.reset_state()\n",
    "        test_probability_accuracy.reset_state()\n",
    "        \n",
    "        \n",
    "        for l_eyes, r_eyes, faces, labels in split_into_batches(ds_train,batch_size) :\n",
    "            l_eyes, r_eyes, faces, labels = np.array(l_eyes), np.array(r_eyes), np.array(faces), np.array(labels)\n",
    "            train_step(l_eyes, r_eyes, faces, labels)\n",
    "\n",
    "        for l_eyes, r_eyes, faces, labels in split_into_batches(ds_test,batch_size) :\n",
    "            l_eyes, r_eyes, faces, labels = np.array(l_eyes), np.array(r_eyes), np.array(faces), np.array(labels)\n",
    "            test_step(l_eyes, r_eyes, faces, labels)\n",
    "            \n",
    "        print(f\"Epoch {epoch} -- END\")\n",
    "        print(\"\\n Training\")\n",
    "        print(f\"FAR-Loss: {train_far_loss_metric.result()}\")   \n",
    "        print(f\"E-Loss: {train_e_loss_metric.result()}\")   \n",
    "        print(f\"Reliability Accuracy: {train_probability_accuracy.result() * 100}%\")   \n",
    "        print(f\"Angular Error: {train_angular_error_metric.result()}\")  \n",
    "        print(\"\\n Validation\")\n",
    "        print(f\"FAR-Loss: {test_far_loss_metric.result()}\")   \n",
    "        print(f\"E-Loss: {test_e_loss_metric.result()}\")   \n",
    "        print(f\"Reliability Accuracy: {test_probability_accuracy.result() * 100}%\")   \n",
    "        print(f\"Angular Error: {test_angular_error_metric.result()}\")  \n",
    "        \n",
    "        train_far_losses.append(train_far_loss_metric.result()) \n",
    "        train_e_losses.append(train_e_loss_metric.result()) \n",
    "        train_choose_accs.append(train_probability_accuracy.result()) \n",
    "        train_angular_errs.append(train_angular_error_metric.result()) \n",
    "        \n",
    "        test_far_losses.append(test_far_loss_metric.result()) \n",
    "        test_e_losses.append(test_e_loss_metric.result()) \n",
    "        test_choose_accs.append(test_probability_accuracy.result()) \n",
    "        test_angular_errs.append(test_angular_error_metric.result()) \n",
    "\n",
    "    \n",
    "    \n",
    "    save_path = f\"log/{ dataset_type }/{epochs}\"\n",
    "\n",
    "    if not os.path.exists(save_path) :\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        \n",
    "    far_net_model.save(f\"{save_path}/far_net_model.keras\")\n",
    "    e_net_model.save(f\"{save_path}/e_net_model.keras\")\n",
    "    print(f\"Model Saved at {save_path}\")\n",
    "    \n",
    "    return train_far_losses, train_e_losses, train_angular_errs, train_choose_accs, test_far_losses, test_e_losses, test_angular_errs, test_choose_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_type = \"enhanced2\"\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_eye_images, r_eye_images, face_images, labels = read_data.load_dataset(dataset_type=dataset_type)\n",
    "\n",
    "l_eye_train, l_eye_test, r_eye_train, r_eye_test, face_train, face_test, labels_train, labels_test = train_test_split(\n",
    "    l_eye_images, r_eye_images, face_images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the shapes of the train and test sets\n",
    "print(\"Left eye train shape:\", l_eye_train.shape)\n",
    "print(\"Right eye train shape:\", r_eye_train.shape)\n",
    "print(\"Face train shape:\", face_train.shape)\n",
    "print(\"Labels train shape:\", labels_train.shape)\n",
    "\n",
    "print(\"Left eye test shape:\", l_eye_test.shape)\n",
    "print(\"Right eye test shape:\", r_eye_test.shape)\n",
    "print(\"Face test shape:\", face_test.shape)\n",
    "print(\"Labels test shape:\", labels_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_far_losses, train_e_losses, train_angular_errs, train_choose_accs, test_far_losses, test_e_losses, test_angular_errs, test_choose_accs =  train(ds_train=[l_eye_train, r_eye_train, face_train, labels_train], ds_test=[l_eye_test, r_eye_test, face_test, labels_test], epochs=epochs, dataset_type=dataset_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_result(epochs, train_result, test_result, y_label, my) :\n",
    "    epochs_range = range(1, epochs + 1)\n",
    "    plt.plot(epochs_range, train_result, label=f\"Training {y_label} \")\n",
    "    plt.plot(epochs_range, test_result, label=f'Validation {y_label}')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(y_label)\n",
    "    plt.legend(loc='upper right')\n",
    "    # plt.xlim(0,mx)\n",
    "    plt.ylim(0,my)\n",
    "    plt.title(f'Training and Validation {y_label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_type = dataset_type\n",
    "save_path = f\"log/{ data_type }/{epochs}\"\n",
    "\n",
    "if not os.path.exists(save_path) :\n",
    "     os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "plt.figure(figsize=(14, 5))    \n",
    "visualize_result(epochs=epochs, train_result=train_far_losses , test_result= test_e_losses, y_label=\"FAR-Loss\", my=1000 )\n",
    "plt.savefig(f\"{save_path}/far-loss.jpg\")\n",
    "plt.figure(figsize=(14, 5))    \n",
    "visualize_result(epochs=epochs, train_result=train_e_losses , test_result= test_e_losses, y_label=\"E-Loss\", my=1000)\n",
    "plt.savefig(f\"{save_path}/e-loss.jpg\")\n",
    "plt.figure(figsize=(14, 5))    \n",
    "visualize_result(epochs=epochs, train_result=train_angular_errs , test_result=test_angular_errs, y_label=\"Angular Error\", my=6)\n",
    "plt.savefig(f\"{save_path}/angular-error.jpg\")\n",
    "plt.figure(figsize=(14, 5))    \n",
    "visualize_result(epochs=epochs, train_result=train_choose_accs , test_result=test_choose_accs, y_label=\"Choose Accuracy\", my=1)\n",
    "plt.savefig(f\"{save_path}/choose-accuracy.jpg\")\n",
    "\n",
    "print(f\"Image saved at {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
