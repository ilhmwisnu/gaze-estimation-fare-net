{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Concatenate, BatchNormalization, Input, Activation, Dropout # type: ignore\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import read_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(filters, strides):\n",
    "    return Conv2D(filters=filters, kernel_size=3, strides=strides, activation='relu', padding='valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eye_conv_blocks(inputs) :\n",
    "    conv1 = conv_block(filters=64, strides=1)(inputs)\n",
    "    conv2 = conv_block(filters=64, strides=2)(conv1)\n",
    "    conv3 = conv_block(filters=128, strides=1)(conv2)\n",
    "    conv4 = conv_block(filters=128, strides=2)(conv3)\n",
    "    conv5 = conv_block(filters=256, strides=1)(conv4)\n",
    "    conv6 = conv_block(filters=256, strides=2)(conv5)\n",
    "    return conv6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def far_net() :\n",
    "    \n",
    "    # First Stream\n",
    "    left_eye_input = Input(shape=(36,60,1))\n",
    "    first_stream = eye_conv_blocks(left_eye_input)\n",
    "    left_eye_features = Flatten()(first_stream)\n",
    "    left_eye_features = Dense(500, activation='relu')(left_eye_features)\n",
    "    \n",
    "    # Second Stream\n",
    "    right_eye_input = Input(shape=(36,60,1))\n",
    "    second_stream = eye_conv_blocks(right_eye_input)\n",
    "    right_eye_features = Flatten()(second_stream)\n",
    "    right_eye_features = Dense(500, activation='relu')(right_eye_features)\n",
    "\n",
    "    # Third Stream\n",
    "    face_input = Input(shape=(224,224,3))\n",
    "    face_features = Conv2D(96, kernel_size=(11,11), strides=(4,4))(face_input)\n",
    "    face_features = BatchNormalization()(face_features)\n",
    "    face_features = Activation(\"relu\")(face_features)\n",
    "    face_features = MaxPooling2D(pool_size=(3,3), strides=(2,2))(face_features)\n",
    "    \n",
    "    face_features = Conv2D(256, kernel_size=(5,5), padding='same')(face_features)\n",
    "    face_features = BatchNormalization()(face_features)\n",
    "    face_features = Activation(\"relu\")(face_features)\n",
    "    face_features = MaxPooling2D(pool_size=(3,3), strides=(2,2))(face_features)\n",
    "    \n",
    "    face_features = Conv2D(384, kernel_size=(3,3), padding='same')(face_features)\n",
    "    face_features = BatchNormalization()(face_features)\n",
    "    face_features = Activation(\"relu\")(face_features)\n",
    "    face_features = Conv2D(384, kernel_size=(3,3), padding='same')(face_features)\n",
    "    face_features = BatchNormalization()(face_features)\n",
    "    face_features = Activation(\"relu\")(face_features)\n",
    "    face_features = Conv2D(256, kernel_size=(3,3), padding='same')(face_features)\n",
    "    face_features = BatchNormalization()(face_features)\n",
    "    face_features = Activation(\"relu\")(face_features)\n",
    "    face_features = MaxPooling2D(pool_size=(3,3), strides=(2,2))(face_features)\n",
    "    \n",
    "    face_features = Flatten()(face_features)\n",
    "    face_features = Dense(4096, activation='relu')(face_features)\n",
    "    face_features = Dropout(0.5)(face_features)\n",
    "    face_features = Dense(4096, activation='relu')(face_features)\n",
    "    face_features = Dropout(0.5)(face_features)\n",
    "    face_features = Dense(500, activation='relu')(face_features)\n",
    "    \n",
    "    concatenated_features = Concatenate()([left_eye_features,right_eye_features, face_features])\n",
    "    outputs = Dense(6)(concatenated_features)\n",
    "    \n",
    "    model = keras.Model(inputs=[left_eye_input,right_eye_input,face_input], outputs=outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_net():\n",
    "    \n",
    "    #First Stream\n",
    "    left_eye_input = Input(shape=(36,60,1))\n",
    "    first_stream = eye_conv_blocks(left_eye_input)\n",
    "    left_eye_features = Flatten()(first_stream)\n",
    "    left_eye_features = Dense(1000, activation='relu')(left_eye_features)\n",
    "    left_eye_features = Dense(500, activation='relu')(left_eye_features)\n",
    "    \n",
    "    #Second Stream\n",
    "    right_eye_input = Input(shape=(36,60,1))\n",
    "    second_stream = eye_conv_blocks(right_eye_input)\n",
    "    right_eye_features = Flatten()(second_stream)\n",
    "    right_eye_features = Dense(1000, activation='relu')(right_eye_features)\n",
    "    right_eye_features = Dense(500, activation='relu')(right_eye_features)\n",
    "    \n",
    "    concatenated_features = Concatenate()([left_eye_features,right_eye_features])\n",
    "    output = Dense(2, activation='softmax')(concatenated_features)\n",
    "    \n",
    "    model = keras.Model(inputs=[left_eye_input, right_eye_input], outputs=output)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_batches(data, batch_size):\n",
    "    batches = []\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch = data[i:i+batch_size]\n",
    "        batches.append(zip(*batch))\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def angular_err(v1, v2):\n",
    "\tv1xv2 = tf.reduce_sum(v1*v2,1)\n",
    "\tv1_len = tf.cast(tf.sqrt(tf.reduce_sum(tf.square(v1), 1)),dtype=tf.float32)\n",
    "\tv2_len = tf.cast(tf.sqrt(tf.reduce_sum(tf.square(v2), 1)),dtype=tf.float32)\n",
    "\t\n",
    "\tval = tf.minimum( v1xv2/((v1_len* v2_len ) + 1e-10), 0.999999)\n",
    " \n",
    "\tdegree = tf.acos(val)\n",
    "\treturn degree * 180/ math.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(ds_train, ds_test, batch_size=100, epochs=1, original=True) :\n",
    "    \n",
    "    ds_train = list(zip(ds_train[0],ds_train[1],ds_train[2],ds_train[3],))\n",
    "    \n",
    "    far_net_model = far_net()\n",
    "    e_net_model = e_net()\n",
    "    \n",
    "    far_optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "    e_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
    "    \n",
    "    angular_errors = []\n",
    "    \n",
    "    far_losses = []\n",
    "    e_losses = []\n",
    "    \n",
    "    for epoch in range(epochs) :\n",
    "        print(f\"Epoch {epoch} -- START\")\n",
    "        \n",
    "        for l_eyes, r_eyes, faces, labels in split_into_batches(ds_train,batch_size) :\n",
    "            \n",
    "            l_eyes, r_eyes, faces, labels = np.array(l_eyes), np.array(r_eyes), np.array(faces), np.array(labels)\n",
    "            \n",
    "            with tf.GradientTape() as far_tape, tf.GradientTape() as e_tape :\n",
    "                \n",
    "                gaze_preds = far_net_model([l_eyes, r_eyes, faces])\n",
    "                probs = e_net_model([l_eyes, r_eyes])\n",
    "                \n",
    "                # print(f\"Gaze  --> {gaze_preds[:,:3]}\")\n",
    "                # print(f\"Label --> {labels[:,:3]}\")\n",
    "                \n",
    "                # print(f\"Gaze  --> {gaze_preds[:,3:]}\")\n",
    "                # print(f\"Label --> {labels[:,3:]}\")\n",
    "                \n",
    "                left_err = angular_err(gaze_preds[:,:3],labels[:,:3])\n",
    "                right_err = angular_err(gaze_preds[:,3:],labels[:,3:])\n",
    "                \n",
    "                # print(f\"Left Err --> {left_err}\")\n",
    "                # print(f\"Right Err --> {right_err}\")\n",
    "                \n",
    "                far_err = ((2 * left_err * right_err ) + 1e-10)  / ((left_err + right_err) + 1e-10)\n",
    "                # print(f\"FAR-err -> {far_err}\")\n",
    "                \n",
    "                avg_err = (left_err + right_err) /2\n",
    "                \n",
    "                n = tf.cast(tf.less_equal(left_err, right_err), tf.float32)\n",
    "                \n",
    "                # print(f\"N -> {n}\")\n",
    "                \n",
    "                squared_distance = tf.reduce_sum(tf.square(left_err - right_err), axis=-1)\n",
    "                e_loss = - (n * squared_distance * tf.math.log(probs[:,0]) + (1 - n) * squared_distance * tf.math.log(probs[:,1]))\n",
    "\n",
    "                \n",
    "                weight = (1 + (2 * n - 1) * probs[:,0] + (1 - 2 * n) * probs[:,1]) / 2\n",
    "                \n",
    "                # print(f\"W --> {weight}\")\n",
    "                \n",
    "                far_loss = weight * far_err + (1 - weight) * 0.1 * avg_err\n",
    "                print(f\"FAR loss --> {tf.reduce_mean(far_loss)}\")\n",
    "                print(f\"E loss --> {tf.reduce_mean(e_loss)}\")\n",
    "                \n",
    "            gradients1 = far_tape.gradient(far_loss, far_net_model.trainable_variables)\n",
    "            gradients2 = e_tape.gradient(e_loss, e_net_model.trainable_variables)\n",
    "            \n",
    "            # print(f\"GRAD --> {gradients1}\")\n",
    "            # print(f\"GRAD --> {gradients2}\")\n",
    "            \n",
    "            far_optimizer.apply_gradients(zip(gradients1, far_net_model.trainable_variables))\n",
    "            e_optimizer.apply_gradients(zip(gradients2, e_net_model.trainable_variables))\n",
    "            \n",
    "            far_losses.append(far_loss)\n",
    "            e_losses.append(e_loss)\n",
    "            \n",
    "        print(f\"Epoch {epoch} -- END\")   \n",
    "        \n",
    "    save_path = \"models/original/\" if original else \"models/enhanced/\"\n",
    "    \n",
    "    if not os.path.exists(save_path) :\n",
    "        os.makedirs(save_path, exist_ok=True) \n",
    "        \n",
    "    far_net_model.save(f\"{save_path}far_net_model.keras\")\n",
    "    e_net_model.save(f\"{save_path}e_net_model.keras\")\n",
    "    print(\"Model Saved\")\n",
    "    \n",
    "    print(\"Validating Model ...\")\n",
    "    \n",
    "    ds_test = list(zip(ds_test[0],ds_test[1],ds_test[2],ds_test[3],))\n",
    "    \n",
    "    angular_errors = []\n",
    "    choose_acc = 0\n",
    "    total = 0\n",
    "    \n",
    "    for l_eyes, r_eyes, faces, labels in split_into_batches(ds_test,batch_size) :\n",
    "    \n",
    "        l_eyes, r_eyes, faces, labels = np.array(l_eyes), np.array(r_eyes), np.array(faces), np.array(labels)\n",
    "    \n",
    "        gaze_preds = far_net_model([l_eyes, r_eyes, faces])\n",
    "        reliability_preds = e_net_model([l_eyes, r_eyes])\n",
    "        \n",
    "        left_err = angular_err(gaze_preds[:,:3],labels[:,:3])\n",
    "        right_err = angular_err(gaze_preds[:,3:],labels[:,3:])\n",
    "        \n",
    "        choose_preds = tf.cast(tf.less_equal(reliability_preds[:,0], reliability_preds[:,1]), tf.int32)\n",
    "        choose_labels = tf.cast(tf.greater(left_err, right_err), tf.int32)\n",
    "        \n",
    "        # print(choose_preds)\n",
    "        # print(choose_labels)\n",
    "        \n",
    "        for i in choose_preds :\n",
    "            if i == 0 :\n",
    "                angular_errors.append(left_err[i])\n",
    "            else :\n",
    "                angular_errors.append(right_err[i])\n",
    "        \n",
    "        \n",
    "        for choose_pred, choose_label in zip(choose_preds,choose_labels):\n",
    "            total += 1\n",
    "            \n",
    "            if choose_pred == choose_label :\n",
    "                choose_acc += 1\n",
    "            \n",
    "    avg_error = tf.reduce_mean(angular_errors)      \n",
    "    choose_acc = choose_acc / total * 100\n",
    "        \n",
    "    \n",
    "    print(f\"Choose Acc => {choose_acc}%\")\n",
    "    print(f\"Angular Error => {avg_error}\")\n",
    "    \n",
    "    return far_losses, e_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_original = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left eye train shape: (1200, 36, 60)\n",
      "Right eye train shape: (1200, 36, 60)\n",
      "Face train shape: (1200, 224, 224, 3)\n",
      "Labels train shape: (1200, 6)\n",
      "Left eye test shape: (300, 36, 60)\n",
      "Right eye test shape: (300, 36, 60)\n",
      "Face test shape: (300, 224, 224, 3)\n",
      "Labels test shape: (300, 6)\n"
     ]
    }
   ],
   "source": [
    "l_eye_images, r_eye_images, face_images, labels = read_data.load_dataset(original_dataset=is_original)\n",
    "\n",
    "l_eye_train, l_eye_test, r_eye_train, r_eye_test, face_train, face_test, labels_train, labels_test = train_test_split(\n",
    "    l_eye_images, r_eye_images, face_images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the shapes of the train and test sets\n",
    "print(\"Left eye train shape:\", l_eye_train.shape)\n",
    "print(\"Right eye train shape:\", r_eye_train.shape)\n",
    "print(\"Face train shape:\", face_train.shape)\n",
    "print(\"Labels train shape:\", labels_train.shape)\n",
    "\n",
    "print(\"Left eye test shape:\", l_eye_test.shape)\n",
    "print(\"Right eye test shape:\", r_eye_test.shape)\n",
    "print(\"Face test shape:\", face_test.shape)\n",
    "print(\"Labels test shape:\", labels_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 -- START\n",
      "FAR loss --> 25.888978958129883\n",
      "E loss --> 262665.71875\n",
      "FAR loss --> 22.811786651611328\n",
      "E loss --> 14134.0087890625\n",
      "FAR loss --> 13.943877220153809\n",
      "E loss --> 95599.8203125\n",
      "FAR loss --> 9.841591835021973\n",
      "E loss --> 108592.90625\n",
      "FAR loss --> 9.609467506408691\n",
      "E loss --> 72072.828125\n",
      "FAR loss --> 9.345309257507324\n",
      "E loss --> 60649.01171875\n",
      "FAR loss --> 8.168403625488281\n",
      "E loss --> 40528.83984375\n",
      "FAR loss --> 8.380247116088867\n",
      "E loss --> 28693.984375\n",
      "FAR loss --> 8.174182891845703\n",
      "E loss --> 18997.431640625\n",
      "FAR loss --> 7.708372592926025\n",
      "E loss --> 11444.2099609375\n",
      "FAR loss --> 6.788550853729248\n",
      "E loss --> 8837.9267578125\n",
      "FAR loss --> 5.556084156036377\n",
      "E loss --> 7105.06884765625\n",
      "Epoch 0 -- END\n",
      "Model Saved\n",
      "Validating Model ...\n",
      "Choose Acc => 46.33333333333333%\n",
      "Angular Error => 11.96943187713623\n"
     ]
    }
   ],
   "source": [
    "far_loss, e_loss =  train(ds_train=[l_eye_train, r_eye_train, face_train, labels_train], ds_test=[l_eye_test, r_eye_test, face_test, labels_test], epochs=1, original=is_original)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
